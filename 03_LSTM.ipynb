{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from math import ceil\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "import random\n",
    "from random import shuffle\n",
    "from dateutil.parser import parse\n",
    "\n",
    "import rl4pm_lib.preprocessing as preprocessing\n",
    "reload(preprocessing)\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('datasets/test_df.csv')# .drop(columns='Unnamed: 0')\n",
    "test_df['timestamp'] = test_df['timestamp'].apply(lambda x: parse(x))\n",
    "\n",
    "train_df = pd.read_csv('datasets/train_df.csv')# .drop(columns='Unnamed: 0')\n",
    "train_df['timestamp'] = train_df['timestamp'].apply(lambda x: parse(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propro like for SklEarn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rl4pm_lib.preprocessing as preprocessing\n",
    "\n",
    "column_feature = {'tt': 0, 'te': 1, 'tw': 2}\n",
    "prepro = preprocessing.DfPreprocesser()\n",
    "prepro.fit(train_df)\n",
    "train_df_pr = prepro.transform(train_df)\n",
    "test_df_pr = prepro.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.PaperScaler(column_feature)\n",
    "scaler.fit(train_df_pr)\n",
    "train_df_pr_sc = scaler.transform(train_df_pr)\n",
    "test_df_pr_sc = scaler.transform(test_df_pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok let's create data loader & dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl4pm_lib.lstm_supervised import ProcessesDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_len = 2\n",
    "batch_size = 512\n",
    "n_lstm = 1\n",
    "m_lstm = 2\n",
    "hidden_layer = 128\n",
    "n_epoch = 100\n",
    "n_classes = len(set(train_df['activity'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = ProcessesDataset(test_df_pr_sc.drop(columns=['trace_id']), win_len)\n",
    "train_ds = ProcessesDataset(train_df_pr_sc, win_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 18])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds[0]['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tt', 'te', 'tw', 'trace_id', 'W_Afhandelen leads',\n",
       "       'W_Beoordelen fraude', 'W_Completeren aanvraag',\n",
       "       'W_Nabellen incomplete dossiers', 'W_Nabellen offertes',\n",
       "       'W_Valideren aanvraag', 'tt__1', 'te__1', 'tw__1',\n",
       "       'W_Afhandelen leads__1', 'W_Beoordelen fraude__1',\n",
       "       'W_Completeren aanvraag__1', 'W_Nabellen incomplete dossiers__1',\n",
       "       'W_Nabellen offertes__1', 'W_Valideren aanvraag__1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds.df_win.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modles init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer, n_lstm, m_lstm, n_classes, dropout=0.05):\n",
    "        super(Net, self).__init__()\n",
    "        if n_lstm > 0:\n",
    "            self.bb_lstm = torch.nn.LSTM(input_size=input_size, hidden_size=hidden_layer,\n",
    "                                         num_layers=n_lstm)\n",
    "            self.ac_lstm = torch.nn.LSTM(input_size=hidden_layer, hidden_size=hidden_layer,\n",
    "                                         num_layers=m_lstm)\n",
    "            self.te_lstm = torch.nn.LSTM(input_size=hidden_layer, hidden_size=hidden_layer,\n",
    "                                         num_layers=m_lstm)\n",
    "        else:\n",
    "            self.ac_lstm = torch.nn.LSTM(input_size=input_size, hidden_size=hidden_layer,\n",
    "                                         num_layers=m_lstm)\n",
    "            self.te_lstm = torch.nn.LSTM(input_size=input_size, hidden_size=hidden_layer,\n",
    "                                         num_layers=m_lstm)\n",
    "            \n",
    "        \n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc_ac = torch.nn.Linear(hidden_layer, n_classes)\n",
    "        self.fc_te = torch.nn.Linear(hidden_layer, 1)\n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "        self.n_lstm = n_lstm\n",
    "        self.m_lstm = m_lstm\n",
    "        self.hidden = hidden_layer\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.n_lstm > 0:\n",
    "            # back bone lstm\n",
    "            x, h = self.bb_lstm(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.relu(x)\n",
    "            if self.n_lstm == self.m_lstm:\n",
    "                h = (self.dropout(h[0]), self.dropout(h[1]))\n",
    "                # head for action and t_e\n",
    "                x_ac, _ = self.ac_lstm(x, h)\n",
    "                x_te, _ = self.te_lstm(x, h)\n",
    "            else:\n",
    "                x_ac, _ = self.ac_lstm(x)\n",
    "                x_te, _ = self.te_lstm(x)\n",
    "        else:\n",
    "            # head for action and t_e\n",
    "            x_ac, _ = self.ac_lstm(x)\n",
    "            x_te, _ = self.te_lstm(x)\n",
    "            \n",
    "\n",
    "        x_te = self.relu(x_te)\n",
    "        x_te = self.relu(x_te)\n",
    "        \n",
    "        x_ac = self.fc_ac(x_ac)\n",
    "        x_te = self.fc_te(x_te)\n",
    "        \n",
    "        x_te = self.relu(x_te)\n",
    "        x_ac = torch.nn.functional.softmax(x_ac, dim=-1)\n",
    "        return x_te, x_ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-5350478d2015>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model = Net(input_size=input_size, hidden_layer=hidden_layer, n_lstm=n_lstm,\n\u001b[0;32m----> 4\u001b[0;31m             m_lstm=m_lstm, n_classes=n_classes).to(device)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_weights_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;31m# Flattens params (on CUDA)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mflatten_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cudnn_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                         self.batch_first, bool(self.bidirectional))  # type: ignore\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device"
     ]
    }
   ],
   "source": [
    "input_size = test_ds.tensor_data.shape[-1]\n",
    "\n",
    "model = Net(input_size=input_size, hidden_layer=hidden_layer, n_lstm=n_lstm,\n",
    "            m_lstm=m_lstm, n_classes=n_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers and losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ac = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "loss_te = torch.nn.SmoothL1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl4pm_lib.lstm_supervised import train_one_epoch, for_evaluate\n",
    "from rl4pm_lib.utils import get_accuracy, get_f1_score, get_log_loss, get_mae_days, plot_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "train_mae = []\n",
    "test_mae = []\n",
    "\n",
    "test_ce = []\n",
    "train_ce = []\n",
    "\n",
    "test_f1 = []\n",
    "train_f1 = []\n",
    "\n",
    "epoches = []\n",
    "\n",
    "for ep in range(n_epoch):\n",
    "    train_acc\n",
    "    train_data = train_one_epoch(dataloader=train_dataloader, device=device,\n",
    "                       model=model, optimizer=optimizer,\n",
    "                       loss_ac=loss_ac, loss_te=loss_te, n_classes=n_classes)\n",
    "    \n",
    "    eval_data = for_evaluate(test_dataloader, model, n_classes=n_classes, device=device)\n",
    "    \n",
    "    epoches.append(ep+1)\n",
    "    \n",
    "    test_acc.append(get_accuracy(true=eval_data['true_label'],\n",
    "                                 pred=eval_data['pred_label']))\n",
    "    train_acc.append(get_accuracy(true=train_data['true_label'],\n",
    "                                  pred=train_data['pred_label']))\n",
    "    \n",
    "    test_mae.append(get_mae_days(true=eval_data['true_tes'],\n",
    "                                 pred=eval_data['pred_tes'], scaler=scaler))\n",
    "    train_mae.append(get_mae_days(true=train_data['true_tes'],\n",
    "                                  pred=train_data['pred_tes'], scaler=scaler))\n",
    "    \n",
    "    test_ce.append(get_log_loss(true=eval_data['true_label'],\n",
    "                                     pred=eval_data['pred_label']))\n",
    "    train_ce.append(get_log_loss(true=train_data['true_label'],\n",
    "                                  pred=train_data['pred_label']))\n",
    "    \n",
    "    test_f1.append(get_f1_score(true=eval_data['true_label'],\n",
    "                                 pred=eval_data['pred_label']))\n",
    "    train_f1.append(get_f1_score(true=train_data['true_label'],\n",
    "                                  pred=train_data['pred_label']))\n",
    "    \n",
    "    display.clear_output(wait=True)\n",
    "    plot_learning(test_acc=test_acc, train_acc=train_acc,\n",
    "                  test_mae=test_mae, train_mae=train_mae,\n",
    "                  test_f1=test_f1, train_f1=train_f1,\n",
    "                  test_ce=test_ce, train_ce=train_ce,\n",
    "                  epoches=epoches\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([1., 2., 3.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
