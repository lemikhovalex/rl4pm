{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from math import ceil\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "import random\n",
    "from random import shuffle\n",
    "from dateutil.parser import parse\n",
    "\n",
    "import rl4pm_lib.preprocessing as preprocessing\n",
    "from rl4pm_lib.preprocessing import PaperScalerPd as PaperScaler\n",
    "reload(preprocessing)\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_len = 2\n",
    "\n",
    "test_df_pr_win = pd.read_csv(f'datasets/test_features_win_{win_len}.csv')\n",
    "train_df_pr_win = pd.read_csv(f'datasets/train_features_win_{win_len}.csv')\n",
    "\n",
    "if 'timestamp' in test_df_pr_win:\n",
    "    test_df_pr_win['timestamp'] = test_df_pr_win['timestamp'].apply(lambda x: parse(x))\n",
    "if 'timestamp' in train_df_pr_win:\n",
    "    train_df_pr_win['timestamp'] = train_df_pr_win['timestamp'].apply(lambda x: parse(x))\n",
    "    \n",
    "test_df_pr_win.sort_values(by=['timestamp'], inplace=True)\n",
    "train_df_pr_win.sort_values(by=['timestamp'], inplace=True)\n",
    "\n",
    "test_labels, test_tes = test_df_pr_win['labels'], test_df_pr_win['te_true']\n",
    "test_df_pr_win = test_df_pr_win.drop(columns=['labels', 'te_true'])\n",
    "\n",
    "train_labels, train_tes = train_df_pr_win['labels'], train_df_pr_win['te_true']\n",
    "train_df_pr_win = train_df_pr_win.drop(columns=['labels', 'te_true'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propro like for SklEarn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_feature = {'tt': 0, 'te': 1, 'tw': 2}\n",
    "scaler = PaperScaler(column_feature, drop_useless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(train_df_pr_win)\n",
    "\n",
    "train_df_pr_win_sc = scaler.transform(train_df_pr_win)\n",
    "test_df_pr_win_sc = scaler.transform(test_df_pr_win)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok let's create data loader & dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl4pm_lib.lstm_supervised import ProcessesDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['W_Afhandelen leads',\n",
       " 'W_Beoordelen fraude',\n",
       " 'W_Completeren aanvraag',\n",
       " 'W_Nabellen incomplete dossiers',\n",
       " 'W_Nabellen offertes',\n",
       " 'W_Valideren aanvraag']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activities = []\n",
    "_d = train_df_pr_win.copy()\n",
    "for _c in _d.columns:\n",
    "    if (_c[:2] == 'W_') and ('__' not in _c):\n",
    "        activities.append(_c)\n",
    "activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "n_lstm = 1\n",
    "m_lstm = 2\n",
    "hidden_layer = 128\n",
    "n_epoch = 100\n",
    "n_classes = len(activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_ds = ProcessesDataset(test_df_pr_win_sc, win_len)\n",
    "train_ds = ProcessesDataset(train_df_pr_win_sc, win_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modles init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer, n_lstm, m_lstm, n_classes, dropout=0.05):\n",
    "        super(Net, self).__init__()\n",
    "        if n_lstm > 0:\n",
    "            self.bb_lstm = torch.nn.LSTM(input_size=input_size, hidden_size=hidden_layer,\n",
    "                                         num_layers=n_lstm)\n",
    "            self.ac_lstm = torch.nn.LSTM(input_size=hidden_layer, hidden_size=hidden_layer,\n",
    "                                         num_layers=m_lstm)\n",
    "            self.te_lstm = torch.nn.LSTM(input_size=hidden_layer, hidden_size=hidden_layer,\n",
    "                                         num_layers=m_lstm)\n",
    "        else:\n",
    "            self.ac_lstm = torch.nn.LSTM(input_size=input_size, hidden_size=hidden_layer,\n",
    "                                         num_layers=m_lstm)\n",
    "            self.te_lstm = torch.nn.LSTM(input_size=input_size, hidden_size=hidden_layer,\n",
    "                                         num_layers=m_lstm)\n",
    "            \n",
    "        \n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc_ac = torch.nn.Linear(hidden_layer, n_classes)\n",
    "        self.fc_te = torch.nn.Linear(hidden_layer, 1)\n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "        self.n_lstm = n_lstm\n",
    "        self.m_lstm = m_lstm\n",
    "        self.hidden = hidden_layer\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.n_lstm > 0:\n",
    "            # back bone lstm\n",
    "            x, h = self.bb_lstm(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.relu(x)\n",
    "            if self.n_lstm == self.m_lstm:\n",
    "                h = (self.dropout(h[0]), self.dropout(h[1]))\n",
    "                # head for action and t_e\n",
    "                x_ac, _ = self.ac_lstm(x, h)\n",
    "                x_te, _ = self.te_lstm(x, h)\n",
    "            else:\n",
    "                x_ac, _ = self.ac_lstm(x)\n",
    "                x_te, _ = self.te_lstm(x)\n",
    "        else:\n",
    "            # head for action and t_e\n",
    "            x_ac, _ = self.ac_lstm(x)\n",
    "            x_te, _ = self.te_lstm(x)\n",
    "            \n",
    "\n",
    "        x_te = self.relu(x_te)\n",
    "        x_te = self.relu(x_te)\n",
    "        \n",
    "        x_ac = self.fc_ac(x_ac)\n",
    "        x_te = self.fc_te(x_te)\n",
    "        \n",
    "        x_te = self.relu(x_te)\n",
    "        return x_te, x_ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = test_ds.tensor_data.shape[-1]\n",
    "\n",
    "model = Net(input_size=input_size, hidden_layer=hidden_layer, n_lstm=n_lstm,\n",
    "            m_lstm=m_lstm, n_classes=n_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers and losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ac = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "loss_te = torch.nn.SmoothL1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl4pm_lib.lstm_supervised import train_one_epoch, for_evaluate\n",
    "from rl4pm_lib.utils import get_accuracy, get_f1_score, get_log_loss, get_mae_days, plot_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-7acc12809397>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mtrain_acc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     train_data = train_one_epoch(dataloader=train_dataloader, device=device,\n\u001b[0m\u001b[0;32m     18\u001b[0m                        \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                        loss_ac=loss_ac, loss_te=loss_te, n_classes=n_classes)\n",
      "\u001b[1;32m~\\Documents\\5th_course\\rl4pm_master\\rl4pm_lib\\lstm_supervised.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[1;34m(dataloader, device, loss_ac, loss_te, optimizer, model, n_classes)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mloss_ac_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_ac\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m         \u001b[0mloss_ac_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pc\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pc\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "train_mae = []\n",
    "test_mae = []\n",
    "\n",
    "test_ce = []\n",
    "train_ce = []\n",
    "\n",
    "test_f1 = []\n",
    "train_f1 = []\n",
    "\n",
    "epoches = []\n",
    "\n",
    "for ep in range(n_epoch):\n",
    "    train_acc\n",
    "    train_data = train_one_epoch(dataloader=train_dataloader, device=device,\n",
    "                       model=model, optimizer=optimizer,\n",
    "                       loss_ac=loss_ac, loss_te=loss_te, n_classes=n_classes)\n",
    "    \n",
    "    eval_data = for_evaluate(test_dataloader, model, n_classes=n_classes, device=device)\n",
    "    \n",
    "    epoches.append(ep+1)\n",
    "    \n",
    "    test_acc.append(get_accuracy(true=eval_data['true_label'],\n",
    "                                 pred=eval_data['pred_label']))\n",
    "    train_acc.append(get_accuracy(true=train_data['true_label'],\n",
    "                                  pred=train_data['pred_label']))\n",
    "    \n",
    "    test_mae.append(get_mae_days(true=eval_data['true_tes'],\n",
    "                                 pred=eval_data['pred_tes'], scaler=scaler))\n",
    "    train_mae.append(get_mae_days(true=train_data['true_tes'],\n",
    "                                  pred=train_data['pred_tes'], scaler=scaler))\n",
    "    \n",
    "    test_ce.append(get_log_loss(true=eval_data['true_label'],\n",
    "                                     pred=eval_data['pred_label']))\n",
    "    train_ce.append(get_log_loss(true=train_data['true_label'],\n",
    "                                  pred=train_data['pred_label']))\n",
    "    \n",
    "    test_f1.append(get_f1_score(true=eval_data['true_label'],\n",
    "                                 pred=eval_data['pred_label']))\n",
    "    train_f1.append(get_f1_score(true=train_data['true_label'],\n",
    "                                  pred=train_data['pred_label']))\n",
    "    \n",
    "    display.clear_output(wait=True)\n",
    "    plot_learning(test_acc=test_acc, train_acc=train_acc,\n",
    "                  test_mae=test_mae, train_mae=train_mae,\n",
    "                  test_f1=test_f1, train_f1=train_f1,\n",
    "                  test_ce=test_ce, train_ce=train_ce,\n",
    "                  epoches=epoches\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([1., 2., 3.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
