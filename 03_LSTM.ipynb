{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from math import ceil\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "import random\n",
    "from random import shuffle\n",
    "from dateutil.parser import parse\n",
    "\n",
    "import rl4pm_lib.preprocessing as preprocessing\n",
    "reload(preprocessing)\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('datasets/test_df_nr.csv')\n",
    "test_df['timestamp'] = test_df['timestamp'].apply(lambda x: parse(x))\n",
    "\n",
    "train_df = pd.read_csv('datasets/train_df_nr.csv')\n",
    "train_df['timestamp'] = train_df['timestamp'].apply(lambda x: parse(x))\n",
    "\n",
    "train_df['activity'] = train_df['activity'].apply(lambda x: str(x))\n",
    "test_df['activity'] = test_df['activity'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create val dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "train_idx = set(train_df['trace_id'].values)\n",
    "val_idx = sample(train_idx, int(0.25 * len(train_idx)))\n",
    "\n",
    "train_idx_ = []\n",
    "for i in train_idx:\n",
    "    if i not in val_idx:\n",
    "        train_idx_.append(i)\n",
    "train_idx = train_idx_\n",
    "\n",
    "val_df = train_df[train_df['trace_id'].isin(val_idx)]\n",
    "train_df = train_df[train_df['trace_id'].isin(train_idx)]\n",
    "\n",
    "train_df.reset_index(inplace=True, drop=True)\n",
    "val_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propro like for SklEarn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rl4pm_lib.preprocessing as preprocessing\n",
    "\n",
    "column_feature = {'tt': 0, 'te': 1, 'tw': 2}\n",
    "prepro = preprocessing.DfPreprocesser()\n",
    "prepro.fit(train_df)\n",
    "train_df_pr = prepro.transform(train_df)\n",
    "test_df_pr = prepro.transform(test_df)\n",
    "val_df_pr = prepro.transform(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'test_df_pr nans = {test_df_pr.isna().sum().sum()}')\n",
    "print(f'train_df_pr nans = {train_df_pr.isna().sum().sum()}')\n",
    "print(f'val_df_pr nans = {val_df_pr.isna().sum().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_pr.rename(columns={i+1: f'activ_{str(i+1)}' for i in range(6)}, inplace=True)\n",
    "test_df_pr.rename(columns={i+1: f'activ_{str(i+1)}' for i in range(6)}, inplace=True)\n",
    "val_df_pr.rename(columns={i+1: f'activ_{str(i+1)}' for i in range(6)}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.PaperScalerPd(column_feature, drop_useless=False)\n",
    "scaler.fit(train_df_pr)\n",
    "train_df_pr_sc = scaler.transform(train_df_pr)\n",
    "test_df_pr_sc = scaler.transform(test_df_pr)\n",
    "val_df_pr_sc = scaler.transform(val_df_pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok let's create data loader & dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl4pm_lib.lstm_supervised import ProcessesDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_len = 2\n",
    "batch_size = 1024\n",
    "n_lstm = 1\n",
    "m_lstm = 2\n",
    "hidden_layer = 128\n",
    "n_epoch = 100\n",
    "n_classes = len(set(train_df['activity'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = ProcessesDataset(test_df_pr_sc, win_len)\n",
    "train_ds = ProcessesDataset(train_df_pr_sc, win_len)\n",
    "val_ds = ProcessesDataset(val_df_pr_sc, win_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modles init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLSTM(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer, n_lstm, dropout=0.05):\n",
    "        super(NLSTM, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.lstms = torch.nn.ModuleList()\n",
    "        self.n_lstm = n_lstm\n",
    "        for i in range(n_lstm):\n",
    "            if i == 0:\n",
    "                self.lstms.append(torch.nn.LSTM(input_size, hidden_layer, 1))\n",
    "            else:\n",
    "                self.lstms.append(torch.nn.LSTM(hidden_layer, hidden_layer, 1))\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h=None):\n",
    "        # print(x.shape)\n",
    "        if self.n_lstm > 0:\n",
    "            for i in range(self.n_lstm):\n",
    "                if h is None:\n",
    "                    x, h = self.lstms[i](x)\n",
    "                else:\n",
    "                    x, h = self.lstms[i](x, h)\n",
    "\n",
    "                x = self.dropout(x)\n",
    "                x = self.relu(x)\n",
    "                \n",
    "            return x, h\n",
    "        else:\n",
    "            return x, None\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer, n_lstm, m_lstm, n_classes, dropout=0.05):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.bb_lstm = NLSTM(input_size, hidden_layer, n_lstm, dropout=dropout)\n",
    "        nlstm_in_size = hidden_layer\n",
    "        if n_lstm == 0:\n",
    "            nlstm_in_size = input_size\n",
    "        self.ac_lstm = NLSTM(input_size=nlstm_in_size, hidden_layer=hidden_layer,\n",
    "                             n_lstm=m_lstm, dropout=dropout)\n",
    "        self.te_lstm = NLSTM(input_size=nlstm_in_size, hidden_layer=hidden_layer,\n",
    "                             n_lstm=m_lstm, dropout=dropout)\n",
    "                    \n",
    "        \n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc_ac = torch.nn.Linear(hidden_layer, n_classes)\n",
    "        self.fc_te = torch.nn.Linear(hidden_layer, 1)\n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "        self.n_lstm = n_lstm\n",
    "        self.m_lstm = m_lstm\n",
    "        self.hidden = hidden_layer\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "            # back bone lstm\n",
    "        x, h = self.bb_lstm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "            \n",
    "        x_ac, _ = self.ac_lstm(x, h)\n",
    "        x_te, _ = self.te_lstm(x, h)\n",
    "           \n",
    "        x_te = self.dropout(x_te)\n",
    "        x_ac = self.dropout(x_ac)\n",
    "        \n",
    "        x_te = self.relu(x_te)\n",
    "        x_ac = self.relu(x_ac)\n",
    "        x_ac = self.fc_ac(x_ac)\n",
    "        x_te = self.fc_te(x_te)\n",
    "        \n",
    "        x_te = self.relu(x_te)\n",
    "        x_ac = torch.nn.functional.softmax(x_ac, dim=-1)\n",
    "        return x_te, x_ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = test_ds.tensor_data.shape[-1]\n",
    "\n",
    "model = Net(input_size=input_size, hidden_layer=hidden_layer, n_lstm=n_lstm,\n",
    "            m_lstm=m_lstm, n_classes=n_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers and losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ac = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "loss_te = torch.nn.SmoothL1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl4pm_lib.lstm_supervised import train_one_epoch, for_evaluate\n",
    "from rl4pm_lib.utils import get_accuracy, get_f1_score, get_log_loss, get_mae_days, plot_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "train_mae = []\n",
    "test_mae = []\n",
    "\n",
    "test_ce = []\n",
    "train_ce = []\n",
    "\n",
    "test_f1 = []\n",
    "train_f1 = []\n",
    "\n",
    "epoches = []\n",
    "\n",
    "for ep in range(160):\n",
    "    \n",
    "    train_data = train_one_epoch(dataloader=train_dataloader, device=device,\n",
    "                       model=model, optimizer=optimizer,\n",
    "                       loss_ac=loss_ac, loss_te=loss_te, n_classes=n_classes)\n",
    "    \n",
    "    eval_data = for_evaluate(val_dataloader, model, n_classes=n_classes, device=device)\n",
    "    \n",
    "    epoches.append(ep+1)\n",
    "    \n",
    "    test_acc.append(get_accuracy(true=eval_data['true_label'],\n",
    "                                 pred=eval_data['pred_label']))\n",
    "    train_acc.append(get_accuracy(true=train_data['true_label'],\n",
    "                                  pred=train_data['pred_label']))\n",
    "    \n",
    "    test_mae.append(get_mae_days(true=eval_data['true_tes'],\n",
    "                                 pred=eval_data['pred_tes'], scaler=scaler))\n",
    "    train_mae.append(get_mae_days(true=train_data['true_tes'],\n",
    "                                  pred=train_data['pred_tes'], scaler=scaler))\n",
    "    \n",
    "    test_ce.append(get_log_loss(true=eval_data['true_label'],\n",
    "                                     pred=eval_data['pred_label']))\n",
    "    train_ce.append(get_log_loss(true=train_data['true_label'],\n",
    "                                  pred=train_data['pred_label']))\n",
    "    \n",
    "    test_f1.append(get_f1_score(true=eval_data['true_label'],\n",
    "                                 pred=eval_data['pred_label']))\n",
    "    train_f1.append(get_f1_score(true=train_data['true_label'],\n",
    "                                  pred=train_data['pred_label']))\n",
    "    \n",
    "    display.clear_output(wait=True)\n",
    "    plot_learning(test_acc=test_acc, train_acc=train_acc,\n",
    "                  test_mae=test_mae, train_mae=train_mae,\n",
    "                  test_f1=test_f1, train_f1=train_f1,\n",
    "                  test_ce=test_ce, train_ce=train_ce,\n",
    "                  epoches=epoches\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper param tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hyper(config, checkpoint_dir='hyper_tune', train_dataloader=train_dataloader, test_dataloader=test_dataloader,\n",
    "                n_classes=6, n_epoches=40):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = Net(input_size=input_size, hidden_layer=config['hidden'], n_lstm=config['n_lstm'],\n",
    "            m_lstm=config['m_lstm'], n_classes=n_classes).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    \n",
    "    loss_ac = torch.nn.CrossEntropyLoss()\n",
    "    loss_te = torch.nn.SmoothL1Loss()\n",
    "    for epoch in range(n_epoches):\n",
    "        train_result = train_one_epoch(dataloader=train_dataloader, device=device, model=model, optimizer=optimizer,\n",
    "                                       loss_ac=loss_ac, loss_te=loss_te, n_classes=n_classes)\n",
    "        eval_data = for_evaluate(test_dataloader, model, n_classes=n_classes, device=device)\n",
    "        \n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((model.state_dict(), optimizer.state_dict()), path)\n",
    "            \n",
    "        val_loss = get_log_loss(true=eval_data['true_label'],\n",
    "                                pred=eval_data['pred_label'])\n",
    "        vall_acc = get_accuracy(true=train_data['true_label'],\n",
    "                                pred=train_data['pred_label'])\n",
    "        tune.report(loss=val_loss, accuracy=vall_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import os\n",
    "from torch.utils.data import random_split\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import ray\n",
    "ray.shutdown()\n",
    "#ray.init(log_to_driver=False)\n",
    "\n",
    "data_dir = os.path.abspath(\"checkpoint\")\n",
    "config = {\n",
    "        \"n_lstm\": tune.choice([1, 2, 3]),\n",
    "        \"m_lstm\": tune.choice([1, 2, 3]),\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"batch_size\": tune.choice([128, 256]),\n",
    "        \"hidden\": tune.choice([128, 200, 300])\n",
    "    }\n",
    "\n",
    "scheduler = ASHAScheduler(\n",
    "        metric=\"accuracy\",\n",
    "        mode=\"max\",\n",
    "        max_t=700*40,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "\n",
    "reporter = CLIReporter(metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "\n",
    "result = tune.run(partial(train_hyper, train_dataloader=train_dataloader,\n",
    "                          test_dataloader=test_dataloader, n_classes=6,\n",
    "                          n_epoches=40\n",
    "                         ),\n",
    "                  resources_per_trial={\"cpu\": 5, \"gpu\": 1},\n",
    "                  num_samples=700,\n",
    "                  scheduler=scheduler,\n",
    "                  config=config,\n",
    "                  progress_reporter=reporter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
