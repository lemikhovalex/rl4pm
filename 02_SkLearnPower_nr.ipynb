{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from IPython import display\n",
    "from dateutil.parser import parse\n",
    "\n",
    "from rl4pm_lib.utils_supervised import make_window_features\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('datasets/test_df_nr.csv')\n",
    "test_df['timestamp'] = test_df['timestamp'].apply(lambda x: parse(x))\n",
    "test_df['trace_id'] = test_df['trace_id'].apply(lambda x: int(x))\n",
    "\n",
    "train_df = pd.read_csv('datasets/train_df_nr.csv')\n",
    "train_df['timestamp'] = train_df['timestamp'].apply(lambda x: parse(x))\n",
    "train_df['trace+id'] = train_df['trace_id'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proprocessing:\n",
    "### 1. Make features\n",
    "For leveraging predictive models the following features are created:\n",
    "- $t_e$ - time since previous event\n",
    "- $t_w$ - time since the beginning of week\n",
    "- $t_t$ - time since the beginning of trace\n",
    "- one hot encoded labels\n",
    "\n",
    "### 2. Make window with lags\n",
    "\n",
    "### 3. Scale it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rl4pm_lib.preprocessing as preprocessing\n",
    "# make features\n",
    "column_feature = {'tt': 0, 'te': 1, 'tw': 2}\n",
    "prepro = preprocessing.DfPreprocesser()\n",
    "prepro.fit(train_df)\n",
    "train_df_pr = prepro.transform(train_df)\n",
    "test_df_pr = prepro.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tt', 'te', 'tw', 'trace_id', 'timestamp', 1, 2, 3, 4, 5, 6], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_pr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_len = 2\n",
    "\n",
    "test_df_pr = test_df_pr.rename(columns={i+1: str(i+1) for i in range(6)})\n",
    "train_df_pr = train_df_pr.rename(columns={i+1: str(i+1) for i in range(6)})\n",
    "\n",
    "test_df_pr_win, test_labels, test_tes = make_window_features(train_df_pr, win_len)\n",
    "\n",
    "train_df_pr_win, train_labels, train_tes = make_window_features(train_df_pr, win_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data above can be just writed to disk so it is fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_write_win_test = test_df_pr_win\n",
    "to_write_win_test['labels'] = test_labels\n",
    "to_write_win_test['te_true'] = test_tes\n",
    "\n",
    "to_write_win_train = train_df_pr_win\n",
    "to_write_win_train['labels'] = train_labels\n",
    "to_write_win_train['te_true'] = train_tes\n",
    "\n",
    "to_write_win_test.to_csv(f'datasets/test_features_win_{win_len}_nr.csv', index=False)\n",
    "to_write_win_train.to_csv(f'datasets/train_features_win_{win_len}_nr.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_pr_win = pd.read_csv(f'datasets/test_features_win_{win_len}_nr.csv')\n",
    "train_df_pr_win = pd.read_csv(f'datasets/train_features_win_{win_len}_nr.csv')\n",
    "\n",
    "if 'timestamp' in test_df_pr_win:\n",
    "    test_df_pr_win['timestamp'] = test_df_pr_win['timestamp'].apply(lambda x: parse(x))\n",
    "if 'timestamp' in train_df_pr_win:\n",
    "    train_df_pr_win['timestamp'] = train_df_pr_win['timestamp'].apply(lambda x: parse(x))\n",
    "    \n",
    "test_df_pr_win.sort_values(by=['timestamp'], inplace=True)\n",
    "train_df_pr_win.sort_values(by=['timestamp'], inplace=True)\n",
    "\n",
    "test_labels, test_tes = test_df_pr_win['labels'], test_df_pr_win['te_true']\n",
    "test_df_pr_win = test_df_pr_win.drop(columns=['labels', 'te_true'])\n",
    "\n",
    "train_labels, train_tes = train_df_pr_win['labels'], train_df_pr_win['te_true']\n",
    "train_df_pr_win = train_df_pr_win.drop(columns=['labels', 'te_true'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activities = []\n",
    "_d = train_df_pr_win.copy()\n",
    "for _c in _d.columns:\n",
    "    if (_c[:2] == 'W_') and ('__' not in _c):\n",
    "        activities.append(_c)\n",
    "activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ('timestamp' in train_df_pr_win) and (type(train_df_pr_win['timestamp'].values[0])==str):\n",
    "    train_df_pr_win['timestamp'] = train_df_pr_win['timestamp'].apply(lambda x: parse(x))\n",
    "if ('timestamp' in test_df_pr_win) and (type(test_df_pr_win['timestamp'].values[0])==str):\n",
    "    test_df_pr_win['timestamp'] = test_df_pr_win['timestamp'].apply(lambda x: parse(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also must scale continious (time related features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9176, 22), (9176,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_pr_win.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl4pm_lib.preprocessing import PaperScalerPd as PaperScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct features as moving window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, features and targets are ready, let's do `.fit()`, `.predict())))))`\n",
    "\n",
    "What methods will I use? Of course all, which were listed [here](https://github.com/lemikhovalex/DA_CV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PipeLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe = Pipeline([('scaler', PaperScaler(column_feature)),\n",
    "                 ('rfc', RandomForestClassifier(n_estimators=30, max_depth=4, random_state=42, n_jobs=-1))\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler',\n",
       "                 <rl4pm_lib.preprocessing.PaperScalerPd object at 0x00000185A3723FD0>),\n",
       "                ('rfc',\n",
       "                 RandomForestClassifier(max_depth=4, n_estimators=30, n_jobs=-1,\n",
       "                                        random_state=42))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(train_df_pr_win.drop(columns=['timestamp']), train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = TimeSeriesSplit(n_splits=7)\n",
    "xs = splitter.split(train_df_pr_win, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 150 candidates, totalling 750 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   29.8s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:  4.7min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = { \n",
    "    'rfc__n_estimators': np.linspace(140, 250, num=15, dtype=int),\n",
    "    'rfc__max_depth' : np.linspace(10, 20, num=10, dtype=int),\n",
    "}\n",
    "\n",
    "model_gs = Pipeline([('scaler', PaperScaler(column_feature)),\n",
    "                     ('rfc', RandomForestClassifier(random_state=42, class_weight='balanced'))\n",
    "                    ])\n",
    "CV_rfc = GridSearchCV(estimator=model_gs, param_grid=param_grid, scoring='accuracy',\n",
    "                      cv=TimeSeriesSplit(n_splits=5), n_jobs=3, verbose=1)\n",
    "CV_rfc.fit(train_df_pr_win, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_rfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_activ_best = CV_rfc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_activ_best = Pipeline([('scaler', PaperScaler(column_feature)),\n",
    "                            ('rfc', RandomForestClassifier(random_state=42, \n",
    "                                                           max_depth=18,\n",
    "                                                           n_estimators=218\n",
    "                                                          ))\n",
    "                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_activ_best = CV_rfc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rfc_activ_best.fit(train_df_pr_win, train_labels)\n",
    "\n",
    "train_labels_hat = rfc_activ_best.predict(train_df_pr_win)\n",
    "test_labels_hat = rfc_activ_best.predict(test_df_pr_win)\n",
    "\n",
    "test_acc_rfc = accuracy_score(test_labels_hat, test_labels)\n",
    "train_acc_rfc = accuracy_score(train_labels_hat, train_labels)\n",
    "\n",
    "print(f'Random Forest Classifier, after grid search')\n",
    "print(f'test  accuracy = {test_acc_rfc: .2f}\\ntrain accuracy = {train_acc_rfc: .2f}')\n",
    "print(f'test  f1 = {f1_score(test_labels, test_labels_hat, average=\"weighted\"): .2f}')\n",
    "print(f'train f1 = {f1_score(train_labels, train_labels_hat, average=\"weighted\"): .2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cm = confusion_matrix(test_labels, test_labels_hat)\n",
    "cm_prob = cm / np.nan_to_num(cm, 0).sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index = activities,\n",
    "                         columns = activities)\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True)\n",
    "plt.title('RFC\\nConfusion matrix')\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('true')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cm = confusion_matrix(test_labels, test_labels_hat)\n",
    "cm_prob = cm / np.nan_to_num(cm, 0).sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "df_cm = pd.DataFrame(cm_prob, index = activities,\n",
    "                         columns = activities)\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True)\n",
    "plt.title('RFC\\nConfusion matrix as probabilities')\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('true')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resuls some how are beter then in article on LSTM approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFC not balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "param_grid_rfcb = { \n",
    "    'rfc__n_estimators': np.linspace(140, 250, num=15, dtype=int),\n",
    "    'rfc__max_depth' : np.linspace(10, 20, num=10, dtype=int),\n",
    "}\n",
    "\n",
    "model_gs_rfcnb = Pipeline([('scaler', PaperScaler(column_feature)),\n",
    "                           ('rfc', RandomForestClassifier(random_state=42))\n",
    "                          ])\n",
    "CV_rfcub = GridSearchCV(estimator=model_gs_rfcnb, param_grid=param_grid, scoring='accuracy',\n",
    "                      cv=TimeSeriesSplit(n_splits=5), n_jobs=3, verbose=1)\n",
    "CV_rfcub.fit(train_df_pr_win, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_rfcub.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_ub_activ_best = CV_rfc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rfc_ub_activ_best.fit(train_df_pr_win, train_labels)\n",
    "\n",
    "train_labels_hat_rfcub = rfc_ub_activ_best.predict(train_df_pr_win)\n",
    "test_labels_hat_rfcub = rfc_ub_activ_best.predict(test_df_pr_win)\n",
    "\n",
    "test_acc_rfcub = accuracy_score(test_labels_hat_rfcub, test_labels)\n",
    "train_acc_rfcub = accuracy_score(train_labels_hat_rfcub, train_labels)\n",
    "\n",
    "print(f'Random Forest Classifier, after grid search')\n",
    "print(f'test  accuracy = {test_acc_rfcub: .2f}\\ntrain accuracy = {train_acc_rfcub: .2f}')\n",
    "print(f'test  f1 = {f1_score(test_labels, test_labels_hat_rfcub, average=\"weighted\"): .2f}')\n",
    "print(f'train f1 = {f1_score(train_labels, train_labels_hat_rfcub, average=\"weighted\"): .2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cm = confusion_matrix(test_labels, train_labels_hat_rfcub)\n",
    "cm_prob = cm / np.nan_to_num(cm, 0).sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index = activities,\n",
    "                         columns = activities)\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True)\n",
    "plt.title('RFC unbalanced\\nConfusion matrix')\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('true')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cm = confusion_matrix(test_labels, test_labels_hat)\n",
    "cm_prob = cm / np.nan_to_num(cm, 0).sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "df_cm = pd.DataFrame(cm_prob, index = activities,\n",
    "                         columns = activities)\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True)\n",
    "plt.title('RFC unbalanced\\nConfusion matrix as probabilities')\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('true')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grad Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "param = {'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "         'num_class': 6,\n",
    "         'tree_method': 'gpu_hist',\n",
    "\n",
    "        }\n",
    "\n",
    "grid_xgb = {'xgb__max_depth': np.linspace(5, 10, num=5, dtype=int),\n",
    "            'xgb__n_estimators': np.linspace(3, 10, num=7, dtype=int),\n",
    "            'xgb__eta': np.linspace(0.001, 0.3, num=15, dtype=float)\n",
    "           }\n",
    "\n",
    "model_gs_xgb = Pipeline([('scaler', PaperScaler(column_feature)),\n",
    "                         ('xgb', xgb.XGBClassifier(**param))\n",
    "                        ])\n",
    "CV_xgb = GridSearchCV(estimator=model_gs_xgb, param_grid=grid_xgb, scoring='accuracy',\n",
    "                      cv=TimeSeriesSplit(n_splits=5), n_jobs=3, verbose=1)\n",
    "CV_xgb.fit(train_df_pr_win, train_labels)\n",
    "\n",
    "print(CV_xgb.best_score_)\n",
    "print(CV_xgb.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf_best = CV_xgb.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf_best = Pipeline([('scaler', PaperScaler(column_feature)),\n",
    "                         ('xgb', xgb.XGBClassifier(objective='multi:softprob',\n",
    "                                                   num_class=6,\n",
    "                                                   tree_method='gpu_hist',\n",
    "                                                   max_depth=8,\n",
    "                                                   n_estimators=10\n",
    "                                                  )\n",
    "                         )]\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf_best.fit(train_df_pr_win, train_labels)\n",
    "\n",
    "train_labels_hat = xgb_clf_best.predict(train_df_pr_win)\n",
    "test_labels_hat = xgb_clf_best.predict(test_df_pr_win)\n",
    "\n",
    "test_acc_xgb = accuracy_score(test_labels_hat, test_labels)\n",
    "train_acc_xgb = accuracy_score(train_labels_hat, train_labels)\n",
    "\n",
    "print(f'XG boost Classifier')\n",
    "print(f'test  accuracy = {test_acc_xgb: .2f}\\ntrain accuracy = {train_acc_xgb: .2f}')\n",
    "\n",
    "print(f'test  f1 = {f1_score(test_labels, test_labels_hat, average=\"weighted\"): .2f}')\n",
    "print(f'train f1 = {f1_score(train_labels, train_labels_hat, average=\"weighted\"): .2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_labels, test_labels_hat)\n",
    "cm_prob = cm / np.nan_to_num(cm, 0).sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index = activities,\n",
    "                         columns = activities)\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True)\n",
    "plt.title('XG boost Classifier\\nConfusion matrix XGB')\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('true')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_labels, test_labels_hat)\n",
    "cm_prob = cm / np.nan_to_num(cm, 0).sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "df_cm = pd.DataFrame(cm_prob, index = activities,\n",
    "                         columns = activities)\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True)\n",
    "plt.title('XG boost Classifier\\nConfusion matrix XGB')\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('true')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification results\n",
    "Classique approaches povides results, which outperforms result, given in article as baseline, but...\n",
    "\n",
    "- As been shown in visualization, there are a lot of loops, model that doesnt consern information on all the trail seems to be useless for end time prediction\n",
    "- Accuracy is what been used for benchmarking in papers. Maybe it is not the best metric? Who knows, nevertheless this is a bench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "param_grid_rfr = { \n",
    "    'rfr__n_estimators': np.linspace(140, 250, num=15, dtype=int),\n",
    "    'rfr__max_depth' : np.linspace(10, 20, num=10, dtype=int),\n",
    "}\n",
    "\n",
    "model_gs_reg = Pipeline([('scaler', PaperScaler(column_feature)),\n",
    "                         ('rfr', RandomForestRegressor(random_state=42))\n",
    "                        ])\n",
    "CV_rfr = GridSearchCV(estimator=model_gs_reg, param_grid=param_grid_rfr, scoring='neg_mean_absolute_error',\n",
    "                      cv=TimeSeriesSplit(n_splits=5), n_jobs=3, verbose=1)\n",
    "CV_rfr.fit(train_df_pr_win, train_tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_rfr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr =  Pipeline([('scaler', PaperScaler(column_feature)),\n",
    "                 ('rfr', RandomForestRegressor(random_state=42,\n",
    "                                               n_estimators=163,\n",
    "                                               max_depth=10\n",
    "                                              ))\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = CV_rfr.best_estimator_\n",
    "rfr.fit(train_df_pr_win, train_tes)\n",
    "train_tes_hat_rfr = rfr.predict(train_df_pr_win)\n",
    "test_tes_hat_rfr = rfr.predict(test_df_pr_win)\n",
    "\n",
    "test_mae_xgb = MAE(test_tes_hat_rfr, test_tes) / 3600. / 24\n",
    "train_mae_xgb = MAE(train_tes_hat_rfr, train_tes) / 3600. / 24\n",
    "\n",
    "print(f'Random Forest Regression')\n",
    "print(f'MAE  accuracy = {test_mae_xgb: .2f}, days\\ntrain MAE = {test_mae_xgb: .2f}, days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(test_tes / 3600. / 24, test_tes_hat_rfr / 3600. / 24, s=0.1)\n",
    "plt.title('Random Forest Regression $t_e$ prediction')\n",
    "plt.xlabel('true, days')\n",
    "plt.ylabel('predicted, days')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist((test_tes - test_tes_hat_rfr) / 3600. / 24, bins=50)\n",
    "plt.xlabel('$t_e$ diff, days')\n",
    "plt.title('Random Forest Regressor\\nTrue $t_e$ - predicted $t_3$')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_dull = train_tes.mean()\n",
    "test_dull = te_dull * np.ones(test_tes.shape)\n",
    "train_dull = te_dull * np.ones(train_tes.shape)\n",
    "\n",
    "test_mae_dull = MAE(test_dull, test_tes) / 3600. / 24\n",
    "train_mae_dull = MAE(train_dull, train_tes) / 3600. / 24\n",
    "\n",
    "print(f'Random Forest Regression')\n",
    "print(f'MAE  accuracy = {test_mae_dull: .2f}, days\\ntrain MAE = {test_mae_dull: .2f}, days')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again better then papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'tree_method': 'gpu_hist'\n",
    "        }\n",
    "\n",
    "grid_xgbr = {'xgbr__max_depth': np.linspace(5, 10, num=5, dtype=int),\n",
    "            'xgbr__n_estimators': np.linspace(3, 10, num=7, dtype=int),\n",
    "            'xgbr__eta': np.linspace(0.001, 0.3, num=15, dtype=float)\n",
    "           }\n",
    "\n",
    "model_gs_xgbr = Pipeline([('scaler', PaperScaler(column_feature)),\n",
    "                         ('xgbr', xgb.XGBClassifier(**param))\n",
    "                        ])\n",
    "CV_xgbr = GridSearchCV(estimator=model_gs_xgbr, param_grid=grid_xgbr, scoring='neg_mean_absolute_error',\n",
    "                      cv=TimeSeriesSplit(n_splits=5), n_jobs=3, verbose=1)\n",
    "CV_xgbr.fit(train_df_pr_win, train_tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_xgbr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbr = CV_xgbr.best_estimator_\n",
    "xgbr.fit(train_df_pr_win, train_tes)\n",
    "train_tes_hat_xgbr = xgbr.predict(train_df_pr_win)\n",
    "test_tes_hat_xgbr = xgbr.predict(test_df_pr_win)\n",
    "\n",
    "test_mae_xgbr = MAE(test_tes_hat_xgbr, test_tes) / 3600. / 24\n",
    "train_mae_xgbr = MAE(train_tes_hat_xgbr, train_tes) / 3600. / 24\n",
    "\n",
    "print(f'Random Forest Regression')\n",
    "print(f'MAE  accuracy = {test_mae_xgbr: .2f}, days\\ntrain MAE = {test_mae_xgbr: .2f}, days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(test_tes / 3600. / 24, test_tes_hat_xgbr / 3600. / 24, s=0.1)\n",
    "plt.title('Random Forest Regressor $t_e$ prediction')\n",
    "plt.xlabel('true, days')\n",
    "plt.ylabel('predicted, days')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGB is not that cool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_ridge = {'ridge__alpha': np.linspace(1., 30., num=100, dtype=float)}\n",
    "\n",
    "model_gs_ridge = Pipeline([('scaler', PaperScaler(column_feature)),\n",
    "                         ('ridge', Ridge())\n",
    "                        ])\n",
    "CV_ridge = GridSearchCV(estimator=model_gs_ridge, param_grid=grid_ridge, scoring='neg_mean_absolute_error',\n",
    "                      cv=TimeSeriesSplit(n_splits=5), n_jobs=6, verbose=1)\n",
    "CV_ridge.fit(train_df_pr_win, train_tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_ridge.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = CV_ridge.best_estimator_\n",
    "ridge.fit(train_df_pr_win, train_tes)\n",
    "train_tes_hat_ridge = ridge.predict(train_df_pr_win)\n",
    "test_tes_hat_ridge = ridge.predict(test_df_pr_win)\n",
    "\n",
    "test_mae_ridge = MAE(test_tes_hat_ridge, test_tes) / 3600. / 24\n",
    "train_mae_ridge = MAE(train_tes_hat_ridge, train_tes) / 3600. / 24\n",
    "\n",
    "print(f'Random Forest Regression')\n",
    "print(f'MAE  accuracy = {test_mae_ridge: .2f}, days\\ntrain MAE = {test_mae_ridge: .2f}, days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(test_tes / 3600. / 24, test_tes_hat_ridge / 3600. / 24, s=0.1)\n",
    "plt.title('Ridge $t_e$ prediction')\n",
    "plt.xlabel('true, days')\n",
    "plt.ylabel('predicted, days')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist((test_tes - test_tes_hat_ridge) / 3600. / 24, bins=50)\n",
    "plt.xlabel('$t_e$ diff, days')\n",
    "plt.title('Ridge\\nTrue $t_e$ - predicted $t_3$')\n",
    "plt.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
